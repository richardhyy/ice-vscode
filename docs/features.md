# Features

ICE offers a set of features designed to enhance your interaction with LLMs directly within VSCode.

## [Chat Providers](chat-providers.md)
Interact with multiple LLM providers, including OpenAI, Anthropic, Google, and more. ICE supports built-in providers and allows for custom integrations, giving you flexibility in choosing the AI model that best suits your needs.

## [Conversation Management](conversation-management.md)
Efficiently organize and maintain your LLM interactions with ICE's conversation management system. Save chats as editable `.chat` files, enabling easy review, version control, and sharing of your AI conversations.

## [Forking and Editing](forking-and-editing.md)
Explore different conversation paths and refine your interactions. Fork conversations to create new branches, edit both user and AI messages, and regenerate responses to optimize your LLM interactions.

## [Inline Configuration](inline-configuration.md)
Modify chat provider settings and switch between providers within a single conversation. This powerful feature allows you to take advantage of the strengths of different models and adjust parameters on the fly.

## [Message Snippets](message-snippets.md)
Save time and maintain consistency in your LLM interactions with message snippets. Quickly insert frequently used prompts or text patterns.

## [Instant Chat](instant-chat.md)
Start conversations with LLMs quickly without creating a new `.chat` file. Instant Chat provides rapid access to LLM capabilities for quick queries and seamless continuation of previous conversations.
